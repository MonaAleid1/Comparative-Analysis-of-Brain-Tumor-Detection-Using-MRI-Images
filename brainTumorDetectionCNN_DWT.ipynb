{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN for Brain Tumor Detection based on GLCM"
      ],
      "metadata": {
        "id": "cRvj6Pw3kEIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the Dataset and Libraries\n",
        "In this section we need to get the dataset. The dataset is available on a github repository. The repository contains images of brain tumor and normal brain images. The author has also performed augmentation on the dataset, and it is also provided. We use the augmented images for our training purpose.\n",
        "<br>Once the repository is cloned, we will import all the required libraries that we need for our problem."
      ],
      "metadata": {
        "id": "F-VmdyODkJ3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git  clone https://github.com/MohamedAliHabib/Brain-Tumor-Detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs5-h3S7PnIc",
        "outputId": "04a3fa00-d845-47b9-82ff-b1641838fbc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Brain-Tumor-Detection' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "from skimage.color import rgb2gray\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import matplotlib.pyplot as plt  # Import plt module for visualization\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import graycoprops, graycomatrix\n",
        "\n"
      ],
      "metadata": {
        "id": "t64WIIkbylWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declaring Variables"
      ],
      "metadata": {
        "id": "WVcLVFZ8kY8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set directory path and parameters\n",
        "data_dir = '/content/Brain-Tumor-Detection/augmented data'\n",
        "categories = ['yes', 'no']  # Names of the categories\n",
        "img_size = 256  # Size of the image after resizing\n",
        "num_classes = len(categories)\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "IpeRdYjSX7e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction\n",
        "Here will define a function named extract_features(). this method takes image path as input and returns the features for each image. Alongwith the feature extraction, the preprocessing on the images is done. <br>\n",
        "In this method, we calculate the DWT (Discrete Wavelet Transform) features for each image so that the features maybe used for the training purposes. The DWT features are calculated based on mean, variance, maximum, minimum, energy, and entropy. We have calculated six features in this example."
      ],
      "metadata": {
        "id": "70w40m9XkOHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the library whihch is used for calculation of the dwt features\n",
        "import pywt\n",
        "# Preprocess images and extract DWT features\n",
        "def extract_features(image_path):\n",
        "    # Load image and resize\n",
        "    img = plt.imread(image_path)\n",
        "    img = resize(img, (img_size, img_size), anti_aliasing=True)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    img_gray = rgb2gray(img)\n",
        "\n",
        "    # Calculate DWT coefficients\n",
        "    cA, (cH, cV, cD) = pywt.dwt2(img_gray, 'haar')\n",
        "\n",
        "    # Extract DWT features\n",
        "    dwt_mean = np.mean(cA)\n",
        "    dwt_var = np.var(cA)\n",
        "    dwt_max = np.max(cA)\n",
        "    dwt_min = np.min(cA)\n",
        "    dwt_energy = np.sum(np.square(cA))\n",
        "    dwt_entropy = -np.sum(np.square(cA) * np.log(np.square(cA)))\n",
        "\n",
        "    # Return features as an array\n",
        "    features = np.array([dwt_mean, dwt_var, dwt_max, dwt_min, dwt_energy, dwt_entropy])\n",
        "    return features"
      ],
      "metadata": {
        "id": "Fvh8yWYaX36E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Features and Preparing for Training\n",
        "In this step, we load each image, and calculate the features using the method defined earlier. <br>\n",
        "Finally, the features are split in train/val categories for training and testing"
      ],
      "metadata": {
        "id": "Dr_r9HeOkdlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and extract features\n",
        "X = []\n",
        "y = []\n",
        "for category in categories:\n",
        "    path = os.path.join(data_dir, category)\n",
        "    for img_name in os.listdir(path):\n",
        "        img_path = os.path.join(path, img_name)\n",
        "        features = extract_features(img_path)\n",
        "        X.append(features)\n",
        "        y.append(categories.index(category))\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1DMDtEKYAAm",
        "outputId": "9c88f23f-0357-4a59-b7cb-13b47b199442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-decf4c99119b>:21: RuntimeWarning: divide by zero encountered in log\n",
            "  dwt_entropy = -np.sum(np.square(cA) * np.log(np.square(cA)))\n",
            "<ipython-input-12-decf4c99119b>:21: RuntimeWarning: invalid value encountered in multiply\n",
            "  dwt_entropy = -np.sum(np.square(cA) * np.log(np.square(cA)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition\n",
        "Here, an example CNN is defined for the explaination purposes. And, this CNN can further be improved for attaining more accuracy."
      ],
      "metadata": {
        "id": "dtIS7qXfkpKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build CNN model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_dim=6))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "1vdKqengYDcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "In this step, we train the model. We run the training for 10 epochs total. The hyperparameters (batch_size and epochs) can be changed for attaining better results."
      ],
      "metadata": {
        "id": "9s-42ZkSkvti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlsNAOEekwKQ",
        "outputId": "36ad86f0-8ed6-4beb-b3ba-8dd115e913de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "104/104 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.5272 - val_loss: nan - val_accuracy: 0.5182\n",
            "Epoch 2/10\n",
            "104/104 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5272 - val_loss: nan - val_accuracy: 0.5182\n",
            "Epoch 3/10\n",
            "104/104 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5272 - val_loss: nan - val_accuracy: 0.5182\n",
            "Epoch 4/10\n",
            "104/104 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5272 - val_loss: nan - val_accuracy: 0.5182\n",
            "Epoch 5/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.5272 - val_loss: nan - val_accuracy: 0.5182\n",
            "Epoch 6/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.5272 - val_loss: nan - val_accuracy: 0.5182\n",
            "Epoch 7/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.5272 - val_loss: nan - val_accuracy: 0.5182\n",
            "Epoch 8/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.5272 - val_loss: nan - val_accuracy: 0.5182\n",
            "Epoch 9/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.5272 - val_loss: nan - val_accuracy: 0.5182\n",
            "Epoch 10/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.5272 - val_loss: nan - val_accuracy: 0.5182\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee5edd14e0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Testing\n",
        "Now the accuracy and loss is calculated for our model. Here, val dataset is used, we can devide our dataset into three categories, train, test, and val as well. "
      ],
      "metadata": {
        "id": "O5EpWzQNkr9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxfn74oiylMm",
        "outputId": "7acb0cb5-adf1-457d-a051-e371caf2f1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.5182\n",
            "Test loss: nan\n",
            "Test accuracy: 0.518159806728363\n"
          ]
        }
      ]
    }
  ]
}