{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN for Brain Tumor Detection based on GLCM"
      ],
      "metadata": {
        "id": "YLjAoYKLfQ1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the Dataset and Libraries\n",
        "In this section we need to get the dataset. The dataset is available on a github repository. The repository contains images of brain tumor and normal brain images. The author has also performed augmentation on the dataset, and it is also provided. We use the augmented images for our training purpose.\n",
        "<br>Once the repository is cloned, we will import all the required libraries that we need for our problem"
      ],
      "metadata": {
        "id": "nZ9MyqNBfqhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git  clone https://github.com/MohamedAliHabib/Brain-Tumor-Detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs5-h3S7PnIc",
        "outputId": "14d45bdc-5d41-4257-e22d-7b3850614c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Brain-Tumor-Detection'...\n",
            "remote: Enumerating objects: 2363, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 2363 (delta 8), reused 4 (delta 4), pack-reused 2352\u001b[K\n",
            "Receiving objects: 100% (2363/2363), 43.21 MiB | 24.90 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Tr629otPldJ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "from skimage.color import rgb2gray\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import matplotlib.pyplot as plt  # Import plt module for visualization\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import graycoprops, graycomatrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declaring Variables"
      ],
      "metadata": {
        "id": "Jawj8VnGgTBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set directory path and parameters\n",
        "data_dir = '/content/Brain-Tumor-Detection/augmented data'\n",
        "categories = ['yes', 'no']  # Names of the categories\n",
        "img_size = 256  # Size of the image after resizing\n",
        "num_classes = len(categories)\n",
        "batch_size = 16\n",
        "\n"
      ],
      "metadata": {
        "id": "ik3w9Pqdf32R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction\n",
        "Here will define a function named extract_features(). this method takes image path as input and returns the features for each image. Alongwith the feature extraction, the preprocessing on the images is done. <br>\n",
        "In this method, we calculate the GLCM features based on contrast, dissimilarity, homogeneity, ASM,and energy."
      ],
      "metadata": {
        "id": "UixfJl4OgYvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess images and extract GLCM features\n",
        "def extract_features(image_path):\n",
        "    # Load image and resize\n",
        "    img = plt.imread(image_path)\n",
        "    img = resize(img, (img_size, img_size), anti_aliasing=True)\n",
        "    \n",
        "    # Convert to grayscale\n",
        "    img_gray = rgb2gray(img)\n",
        "    \n",
        "    # Calculate GLCM\n",
        "    glcm = graycomatrix((img_gray * 255).astype('uint8'), distances=[5], angles=[0], levels=256, symmetric=True, normed=True)\n",
        "    \n",
        "    # Extract GLCM properties\n",
        "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
        "    dissimilarity = graycoprops(glcm, 'dissimilarity')[0, 0]\n",
        "    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
        "    ASM = graycoprops(glcm, 'ASM')[0, 0]\n",
        "    energy = graycoprops(glcm, 'energy')[0, 0]\n",
        "    correlation = graycoprops(glcm, 'correlation')[0, 0]\n",
        "    \n",
        "    # Return features as an array\n",
        "    features = np.array([contrast, dissimilarity, homogeneity, ASM, energy, correlation])\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "tO70gxtKf7dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Features and Preparing for Training\n",
        "In this step, we load each image, and calculate the features using the method defined earlier. <br>\n",
        "Finally, the features are split in train/val categories for training and testing"
      ],
      "metadata": {
        "id": "fEBcUqOLgecl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and extract features\n",
        "X = []\n",
        "y = []\n",
        "for category in categories:\n",
        "    path = os.path.join(data_dir, category)\n",
        "    for img_name in os.listdir(path):\n",
        "        img_path = os.path.join(path, img_name)\n",
        "        features = extract_features(img_path)\n",
        "        X.append(features)\n",
        "        y.append(categories.index(category))\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "ei9qpEpdf-pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition\n",
        "Here, an example CNN is defined for the explaination purposes. And, this CNN can further be improved for attaining more accuracy."
      ],
      "metadata": {
        "id": "bMGAn4twgkUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build CNN model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_dim=6))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "EZEvnV7rgCit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "In this step, we train the model. We run the training for 10 epochs total. The hyperparameters (batch_size and epochs) can be changed for attaining better results."
      ],
      "metadata": {
        "id": "aLKwklv0gn1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqZMti2OgEiD",
        "outputId": "4095b77d-750c-486d-f865-49fe971886f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "104/104 [==============================] - 2s 6ms/step - loss: 8.0001 - accuracy: 0.5054 - val_loss: 1.3477 - val_accuracy: 0.5182\n",
            "Epoch 2/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 2.5432 - accuracy: 0.5073 - val_loss: 3.0371 - val_accuracy: 0.4818\n",
            "Epoch 3/10\n",
            "104/104 [==============================] - 0s 3ms/step - loss: 2.2370 - accuracy: 0.5363 - val_loss: 3.6486 - val_accuracy: 0.4818\n",
            "Epoch 4/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 2.1919 - accuracy: 0.5224 - val_loss: 1.0930 - val_accuracy: 0.5182\n",
            "Epoch 5/10\n",
            "104/104 [==============================] - 0s 3ms/step - loss: 2.1343 - accuracy: 0.5030 - val_loss: 5.9019 - val_accuracy: 0.4818\n",
            "Epoch 6/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 2.3902 - accuracy: 0.5115 - val_loss: 10.9191 - val_accuracy: 0.4818\n",
            "Epoch 7/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 3.7719 - accuracy: 0.5073 - val_loss: 2.1524 - val_accuracy: 0.4818\n",
            "Epoch 8/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 1.6871 - accuracy: 0.5260 - val_loss: 0.6585 - val_accuracy: 0.5860\n",
            "Epoch 9/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 2.5239 - accuracy: 0.5000 - val_loss: 2.3649 - val_accuracy: 0.5182\n",
            "Epoch 10/10\n",
            "104/104 [==============================] - 0s 2ms/step - loss: 2.2563 - accuracy: 0.5073 - val_loss: 0.6771 - val_accuracy: 0.5956\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7e161a590>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Testing\n",
        "Now the accuracy and loss is calculated for our model. Here, val dataset is used, we can devide our dataset into three categories, train, test, and val as well. "
      ],
      "metadata": {
        "id": "D60tf2f7gq-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZm-mJNuQwTf",
        "outputId": "0ebb5c49-5b04-4d61-ca16-940baae60c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 2ms/step - loss: 0.6771 - accuracy: 0.5956\n",
            "Test loss: 0.6770885586738586\n",
            "Test accuracy: 0.5956416726112366\n"
          ]
        }
      ]
    }
  ]
}